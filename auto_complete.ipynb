{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitsansoffair/auto_complete/blob/master/auto_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWjyron_FvdQ"
      },
      "source": [
        "# auto complete"
      ],
      "id": "kWjyron_FvdQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCr0nqGsFvdb",
        "outputId": "f0c62968-85fd-4663-9a77-47a7a5f22d3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import w3_unittest\n",
        "nltk.data.path.append('.')"
      ],
      "id": "vCr0nqGsFvdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzLkTLklFvdf",
        "outputId": "ab00b248-9e2b-479c-bbc0-cd5b178a7033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of letters: 3335477\n",
            "First 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------\n",
            "Last 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------\n"
          ]
        }
      ],
      "source": [
        "with open(\"./data/en_US.twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "print(\"Number of letters:\", len(data))\n",
        "print(\"First 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[0:300])\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"Last 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[-300:])\n",
        "print(\"-------\")"
      ],
      "id": "lzLkTLklFvdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfAg9j8Fvdg"
      },
      "source": [
        "#### Split data to sentences."
      ],
      "id": "kHfAg9j8Fvdg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsZz7TWYFvdg"
      },
      "outputs": [],
      "source": [
        "def split_to_sentences(data):\n",
        "    sentences = data.split('\\n')\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    return sentences"
      ],
      "id": "CsZz7TWYFvdg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUCHzfz0Fvdh",
        "outputId": "784dd868-68d6-486b-d5bf-2764796b27da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "I have a pen.\n",
            "I have an apple. \n",
            "Ah\n",
            "Apple pen.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = \"\"\"\n",
        "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
        "\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_to_sentences(x)"
      ],
      "id": "uUCHzfz0Fvdh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1oNQA3iFvdh"
      },
      "source": [
        "#### Tokenize sentences procedure."
      ],
      "id": "f1oNQA3iFvdh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U2ov_ThFvdi"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentences(sentences):\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        words = sentence.split(' ')\n",
        "        tokenized, tokenized_words = [], []\n",
        "        for index in range(len(words)):\n",
        "            word = words[index]\n",
        "            if len(word) == 0:\n",
        "                continue\n",
        "            if word[-3:] == \"...\":\n",
        "                tokenized_words.append(word[:-3]), tokenized_words.append(word[-3:])\n",
        "            elif word[-1] in list(\".;?\"):\n",
        "                tokenized_words.append(word[:-1]), tokenized_words.append(word[-1])\n",
        "                index += 1\n",
        "            else:\n",
        "                tokenized_words.append(word)\n",
        "        for word in tokenized_words:\n",
        "            if word == '':\n",
        "                continue\n",
        "            tokenized.append(word)\n",
        "        tokenized_sentences.append(tokenized)\n",
        "    return tokenized_sentences"
      ],
      "id": "0U2ov_ThFvdi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtnI8wFtFvdk",
        "outputId": "a96953b1-ced6-49d5-965d-8c44d2b3f98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green', '.'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ],
      "id": "jtnI8wFtFvdk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3N8ce0CFvdn"
      },
      "source": [
        "#### Get tokenized data procedure."
      ],
      "id": "a3N8ce0CFvdn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCI-midYFvdo"
      },
      "outputs": [],
      "source": [
        "def get_tokenized_data(data):\n",
        "    sentences = data.split('\\n')\n",
        "    reduced = []\n",
        "    for sentence in sentences:\n",
        "        if len(sentence) == 0 or sentence.isspace():\n",
        "            continue\n",
        "        reduced.append(sentence)\n",
        "    tokenized_sentences = tokenize_sentences(reduced)\n",
        "    return tokenized_sentences"
      ],
      "id": "fCI-midYFvdo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS4wg2u3Fvdp",
        "outputId": "3a47d2c3-ffce-4d0b-c272-9561dee5467e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['sky', 'is', 'blue', '.'],\n",
              " ['leaves', 'are', 'green'],\n",
              " ['roses', 'are', 'red', '.']]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
        "get_tokenized_data(x)"
      ],
      "id": "JS4wg2u3Fvdp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkUEg-G4Fvdp"
      },
      "source": [
        "#### Split data into train and test sets."
      ],
      "id": "CkUEg-G4Fvdp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z-dFBWIFvdq"
      },
      "outputs": [],
      "source": [
        "tokenized_data = get_tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)\n",
        "\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ],
      "id": "1z-dFBWIFvdq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U21CPombFvdq",
        "outputId": "dcecaaa3-eb22-4fd6-f5c3-10e1ae9ff8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47961 data are split into 38368 train and 9593 test set\n",
            "First training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "First test sample:\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!!', '>>>>>>>']\n"
          ]
        }
      ],
      "source": [
        "print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(tokenized_data), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "      \n",
        "print(\"First test sample:\")\n",
        "print(test_data[0])"
      ],
      "id": "U21CPombFvdq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkXKS2jjFvds"
      },
      "source": [
        "#### Count words procedure."
      ],
      "id": "KkXKS2jjFvds"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqKpb7WEFvdt"
      },
      "outputs": [],
      "source": [
        "def count_words(tokenized_sentences):\n",
        "    word_counts = {}\n",
        "    for sentence in tokenized_sentences:\n",
        "        for token in sentence:\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    return word_counts"
      ],
      "id": "EqKpb7WEFvdt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB2Nh-UMFvdu",
        "outputId": "1a6f4cf3-466d-4e09-b595-09a7ca5ce6d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sky': 1,\n",
              " 'is': 1,\n",
              " 'blue': 1,\n",
              " '.': 3,\n",
              " 'leaves': 1,\n",
              " 'are': 2,\n",
              " 'green': 1,\n",
              " 'roses': 1,\n",
              " 'red': 1}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "count_words(tokenized_sentences)"
      ],
      "id": "LB2Nh-UMFvdu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC1LmYyvFvdv"
      },
      "source": [
        "#### Get words with frequency above threshold procedure."
      ],
      "id": "oC1LmYyvFvdv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODBaJT8nFvdv"
      },
      "outputs": [],
      "source": [
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "    closed_vocab = []\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    for word, cnt in word_counts.items():\n",
        "        if cnt >= count_threshold:\n",
        "            closed_vocab.append(word)\n",
        "    return closed_vocab"
      ],
      "id": "ODBaJT8nFvdv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMGJePg0Fvdv",
        "outputId": "ed291c50-50d9-4db5-c6f1-2bedd7af19be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closed vocabulary:\n",
            "['.', 'are']\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
        "print(f\"Closed vocabulary:\")\n",
        "print(tmp_closed_vocab)"
      ],
      "id": "SMGJePg0Fvdv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv6kqCg4Fvdw"
      },
      "source": [
        "#### Replace oov words by unknown procedure."
      ],
      "id": "cv6kqCg4Fvdw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5-gTz1uFvdw"
      },
      "outputs": [],
      "source": [
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    vocabulary = set(vocabulary)\n",
        "    replaced_tokenized_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        replaced_sentence = []\n",
        "        for token in sentence:\n",
        "            if token in vocabulary:\n",
        "                replaced_sentence.append(token)\n",
        "            else:\n",
        "                replaced_sentence.append(unknown_token)\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "    return replaced_tokenized_sentences"
      ],
      "id": "_5-gTz1uFvdw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwR9KWS9Fvdx",
        "outputId": "c623d01e-9c22-4a92-f739-fd23b4385fca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence:\n",
            "[['dogs', 'run'], ['cats', 'sleep']]\n",
            "tokenized_sentences with less frequent words converted to '<unk>':\n",
            "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
        "vocabulary = [\"dogs\", \"sleep\"]\n",
        "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
        "print(f\"Original sentence:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ],
      "id": "GwR9KWS9Fvdx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErZPVpm7Fvdx"
      },
      "source": [
        "#### Preprocess data procedure."
      ],
      "id": "ErZPVpm7Fvdx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn-X28ZyFvdy"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\",\n",
        "                    get_words_with_nplus_frequency=get_words_with_nplus_frequency,\n",
        "                    replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
        "    vocabulary = get_words_with_nplus_frequency(tokenized_sentences=train_data, count_threshold=count_threshold)\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
        "    return train_data_replaced, test_data_replaced, vocabulary"
      ],
      "id": "Zn-X28ZyFvdy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20CuXnqSFvdy",
        "outputId": "e41ee637-b2d7-4e43-da9d-c0c024d697f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tmp_train_repl\n",
            "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
            "\n",
            "tmp_test_repl\n",
            "[['<unk>', 'are', '<unk>', '.']]\n",
            "\n",
            "tmp_vocab\n",
            "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
          ]
        }
      ],
      "source": [
        "tmp_train = [['sky', 'is', 'blue', '.'],\n",
        "     ['leaves', 'are', 'green']]\n",
        "tmp_test = [['roses', 'are', 'red', '.']]\n",
        "\n",
        "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
        "                                                           tmp_test, \n",
        "                                                           count_threshold = 1\n",
        "                                                          )\n",
        "\n",
        "print(\"tmp_train_repl\")\n",
        "print(tmp_train_repl)\n",
        "print()\n",
        "print(\"tmp_test_repl\")\n",
        "print(tmp_test_repl)\n",
        "print()\n",
        "print(\"tmp_vocab\")\n",
        "print(tmp_vocab)"
      ],
      "id": "20CuXnqSFvdy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmZr13eQFvdz"
      },
      "source": [
        "#### Preprocess the train and test data."
      ],
      "id": "vmZr13eQFvdz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgNUUceZFvd0"
      },
      "outputs": [],
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
        "                                                                        test_data, \n",
        "                                                                        minimum_freq)"
      ],
      "id": "xgNUUceZFvd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XStD-n8Fvd1",
        "outputId": "353ead73-0e2c-4042-ce24-1feb9eac78aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First preprocessed training sample:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', '<unk>', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n",
            "\n",
            "First preprocessed test sample:\n",
            "['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!!', '<unk>']\n",
            "\n",
            "First 10 vocabulary:\n",
            "['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'of', 'the', 'team']\n",
            "\n",
            "Size of vocabulary: 17720\n"
          ]
        }
      ],
      "source": [
        "print(\"First preprocessed training sample:\")\n",
        "print(train_data_processed[0])\n",
        "print()\n",
        "print(\"First preprocessed test sample:\")\n",
        "print(test_data_processed[0])\n",
        "print()\n",
        "print(\"First 10 vocabulary:\")\n",
        "print(vocabulary[0:10])\n",
        "print()\n",
        "print(\"Size of vocabulary:\", len(vocabulary))"
      ],
      "id": "3XStD-n8Fvd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPNgQZ_6Fvd3"
      },
      "source": [
        "#### Count n-grams procedure."
      ],
      "id": "vPNgQZ_6Fvd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XaZU0p9Fvd6"
      },
      "outputs": [],
      "source": [
        "def build(sentence, index, n):\n",
        "    n_gram = []\n",
        "    for plus in range(n):\n",
        "        n_gram.append(sentence[index + plus])\n",
        "    return tuple(n_gram)\n",
        "\n",
        "def count_n_grams(data, n, start_token='<s>', end_token='<e>'):\n",
        "    n_grams = {}\n",
        "    for sentence in data:\n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "        for index in range(len(sentence) - n + 1):\n",
        "            n_gram = build(sentence, index, n)\n",
        "            if n_gram in n_grams.keys():\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "    return n_grams"
      ],
      "id": "4XaZU0p9Fvd6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue27ckFoFvd8",
        "outputId": "38256dc5-df55-4d13-a8fe-6f9527cf1f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uni-gram:\n",
            "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
            "Bi-gram:\n",
            "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(sentences, 2))"
      ],
      "id": "Ue27ckFoFvd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPnwGiKVFvd9"
      },
      "source": [
        "#### Estimate probability procedure."
      ],
      "id": "VPnwGiKVFvd9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_4qNjJ0Fvd9"
      },
      "outputs": [],
      "source": [
        "def estimate_probability(word, previous_n_gram,\n",
        "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts.keys() else 0\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "    n_plus1_gram = tuple(list(previous_n_gram) + [word])\n",
        "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts.keys() else 0\n",
        "    numerator = n_plus1_gram_count + k\n",
        "    probability = numerator / denominator\n",
        "    return probability"
      ],
      "id": "p_4qNjJ0Fvd9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKBv0bsPFvd-",
        "outputId": "1f265138-27fe-4e6c-8353-d3c9e29abd81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
      ],
      "id": "yKBv0bsPFvd-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku6fyCpZFvd_"
      },
      "source": [
        "#### Estimate probabilities for all words procedure."
      ],
      "id": "Ku6fyCpZFvd_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCQjGECcFvd_"
      },
      "outputs": [],
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, \n",
        "                           end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
        "    previous_n_gram = tuple(previous_n_gram)    \n",
        "    \n",
        "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
        "    vocabulary_size = len(vocabulary)    \n",
        "    \n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram, \n",
        "                                           n_gram_counts, n_plus1_gram_counts, \n",
        "                                           vocabulary_size, k=k)\n",
        "                \n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ],
      "id": "tCQjGECcFvd_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrEy-h32FveA",
        "outputId": "577b34bd-3f06-49ba-e731-0315e132f1bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dog': 0.09090909090909091,\n",
              " 'is': 0.09090909090909091,\n",
              " 'this': 0.09090909090909091,\n",
              " 'a': 0.09090909090909091,\n",
              " 'i': 0.09090909090909091,\n",
              " 'cat': 0.2727272727272727,\n",
              " 'like': 0.09090909090909091,\n",
              " '<e>': 0.09090909090909091,\n",
              " '<unk>': 0.09090909090909091}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
      ],
      "id": "NrEy-h32FveA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21gvapaVFveA",
        "outputId": "c80cbb1f-c460-4e85-cf41-b09588df7c4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dog': 0.09090909090909091,\n",
              " 'is': 0.09090909090909091,\n",
              " 'this': 0.18181818181818182,\n",
              " 'a': 0.09090909090909091,\n",
              " 'i': 0.18181818181818182,\n",
              " 'cat': 0.09090909090909091,\n",
              " 'like': 0.09090909090909091,\n",
              " '<e>': 0.09090909090909091,\n",
              " '<unk>': 0.09090909090909091}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
      ],
      "id": "21gvapaVFveA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1bj6n4QFveA"
      },
      "source": [
        "#### Count and probability matrices procedure."
      ],
      "id": "z1bj6n4QFveA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV-W_zukFveB"
      },
      "outputs": [],
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    \n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]        \n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "    \n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}    \n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}    \n",
        "    \n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "    \n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ],
      "id": "bV-W_zukFveB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i80hoK25FveB",
        "outputId": "eebc2674-0fcc-4445-f0f5-877d481af6e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigram counts\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>is</th>\n",
              "      <th>this</th>\n",
              "      <th>a</th>\n",
              "      <th>i</th>\n",
              "      <th>cat</th>\n",
              "      <th>like</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(is,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(i,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(this,)</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(dog,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(like,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(cat,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(a,)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         dog   is  this    a    i  cat  like  <e>  <unk>\n",
              "(is,)    0.0  0.0   0.0  0.0  0.0  0.0   1.0  0.0    0.0\n",
              "(i,)     0.0  0.0   0.0  0.0  0.0  0.0   1.0  0.0    0.0\n",
              "(this,)  1.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n",
              "(dog,)   0.0  1.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n",
              "(<s>,)   0.0  0.0   1.0  0.0  1.0  0.0   0.0  0.0    0.0\n",
              "(like,)  0.0  0.0   0.0  2.0  0.0  0.0   0.0  0.0    0.0\n",
              "(cat,)   0.0  0.0   0.0  0.0  0.0  0.0   0.0  2.0    0.0\n",
              "(a,)     0.0  0.0   0.0  0.0  0.0  2.0   0.0  0.0    0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "print('bigram counts')\n",
        "display(make_count_matrix(bigram_counts, unique_words))"
      ],
      "id": "i80hoK25FveB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNPZYvzUFveC",
        "outputId": "79f729d4-2ae6-4de5-f121-dad312a085b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "trigram counts\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>is</th>\n",
              "      <th>this</th>\n",
              "      <th>a</th>\n",
              "      <th>i</th>\n",
              "      <th>cat</th>\n",
              "      <th>like</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(this, dog)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(a, cat)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(dog, is)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;, i)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(like, a)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(is, like)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(i, like)</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;, this)</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             dog   is  this    a    i  cat  like  <e>  <unk>\n",
              "(this, dog)  0.0  1.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n",
              "(<s>, <s>)   0.0  0.0   1.0  0.0  1.0  0.0   0.0  0.0    0.0\n",
              "(a, cat)     0.0  0.0   0.0  0.0  0.0  0.0   0.0  2.0    0.0\n",
              "(dog, is)    0.0  0.0   0.0  0.0  0.0  0.0   1.0  0.0    0.0\n",
              "(<s>, i)     0.0  0.0   0.0  0.0  0.0  0.0   1.0  0.0    0.0\n",
              "(like, a)    0.0  0.0   0.0  0.0  0.0  2.0   0.0  0.0    0.0\n",
              "(is, like)   0.0  0.0   0.0  1.0  0.0  0.0   0.0  0.0    0.0\n",
              "(i, like)    0.0  0.0   0.0  1.0  0.0  0.0   0.0  0.0    0.0\n",
              "(<s>, this)  1.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('\\ntrigram counts')\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "display(make_count_matrix(trigram_counts, unique_words))"
      ],
      "id": "jNPZYvzUFveC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nVy6j9dFveC"
      },
      "source": [
        "#### make probability matrix procedure."
      ],
      "id": "-nVy6j9dFveC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wYYoeZIFveD"
      },
      "outputs": [],
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ],
      "id": "5wYYoeZIFveD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnYG5p7VFveD",
        "outputId": "8b33d422-ab89-41ac-f7f0-28a8f080b494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigram probabilities\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>is</th>\n",
              "      <th>this</th>\n",
              "      <th>a</th>\n",
              "      <th>i</th>\n",
              "      <th>cat</th>\n",
              "      <th>like</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(is,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(i,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(this,)</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(dog,)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(like,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(cat,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(a,)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              dog        is      this         a         i       cat      like  \\\n",
              "(is,)    0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
              "(i,)     0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
              "(this,)  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
              "(dog,)   0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
              "(<s>,)   0.090909  0.090909  0.181818  0.090909  0.181818  0.090909  0.090909   \n",
              "(like,)  0.090909  0.090909  0.090909  0.272727  0.090909  0.090909  0.090909   \n",
              "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
              "(a,)     0.090909  0.090909  0.090909  0.090909  0.090909  0.272727  0.090909   \n",
              "\n",
              "              <e>     <unk>  \n",
              "(is,)    0.100000  0.100000  \n",
              "(i,)     0.100000  0.100000  \n",
              "(this,)  0.100000  0.100000  \n",
              "(dog,)   0.100000  0.100000  \n",
              "(<s>,)   0.090909  0.090909  \n",
              "(like,)  0.090909  0.090909  \n",
              "(cat,)   0.272727  0.090909  \n",
              "(a,)     0.090909  0.090909  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "print(\"bigram probabilities\")\n",
        "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
      ],
      "id": "DnYG5p7VFveD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxevmyvoFveE",
        "outputId": "baa3c0be-13e2-47d2-bb2c-bcb033db86b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trigram probabilities\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>is</th>\n",
              "      <th>this</th>\n",
              "      <th>a</th>\n",
              "      <th>i</th>\n",
              "      <th>cat</th>\n",
              "      <th>like</th>\n",
              "      <th>&lt;e&gt;</th>\n",
              "      <th>&lt;unk&gt;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(this, dog)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(a, cat)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(dog, is)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;, i)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(like, a)</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(is, like)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(i, like)</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(&lt;s&gt;, this)</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  dog        is      this         a         i       cat  \\\n",
              "(this, dog)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
              "(<s>, <s>)   0.090909  0.090909  0.181818  0.090909  0.181818  0.090909   \n",
              "(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
              "(dog, is)    0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
              "(<s>, i)     0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
              "(like, a)    0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
              "(is, like)   0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
              "(i, like)    0.100000  0.100000  0.100000  0.200000  0.100000  0.100000   \n",
              "(<s>, this)  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
              "\n",
              "                 like       <e>     <unk>  \n",
              "(this, dog)  0.100000  0.100000  0.100000  \n",
              "(<s>, <s>)   0.090909  0.090909  0.090909  \n",
              "(a, cat)     0.090909  0.272727  0.090909  \n",
              "(dog, is)    0.200000  0.100000  0.100000  \n",
              "(<s>, i)     0.200000  0.100000  0.100000  \n",
              "(like, a)    0.090909  0.090909  0.090909  \n",
              "(is, like)   0.100000  0.100000  0.100000  \n",
              "(i, like)    0.100000  0.100000  0.100000  \n",
              "(<s>, this)  0.100000  0.100000  0.100000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"trigram probabilities\")\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
      ],
      "id": "NxevmyvoFveE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCFE3__QFveF"
      },
      "source": [
        "#### Calculate perplexity procedure."
      ],
      "id": "dCFE3__QFveF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elGBDZznFveG"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>',\n",
        "                         end_token='<e>', k=1.0):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    sentence = [start_token] * n + sentence + [end_token]\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)\n",
        "    product_pi = 1.0\n",
        "    for t in range(n, N):\n",
        "        n_gram = sentence[t - n:t]\n",
        "        word = sentence[t]\n",
        "        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
        "        product_pi *= 1 / probability\n",
        "    perplexity = (product_pi) ** (1 / N)\n",
        "    return perplexity"
      ],
      "id": "elGBDZznFveG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHgIcM_xFveH",
        "outputId": "616863d7-1b01-41cd-f785-06a9c60276a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity for first train sample: 2.8040\n",
            "Perplexity for test sample: 3.9654\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "perplexity_train = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
        "\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_perplexity(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
      ],
      "id": "GHgIcM_xFveH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRm6DyulFveI"
      },
      "source": [
        "#### Suggest a word procedure."
      ],
      "id": "aRm6DyulFveI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV2TB2eHFveI"
      },
      "outputs": [],
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>',\n",
        "                   unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "    suggestion = None\n",
        "    max_prob = 0\n",
        "    for word, prob in probabilities.items():\n",
        "        if start_with is not None:\n",
        "            if start_with not in word or word.index(start_with) != 0:\n",
        "                continue\n",
        "        if prob > max_prob:\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "    return suggestion, max_prob"
      ],
      "id": "dV2TB2eHFveI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9yyvcv9FveK",
        "outputId": "e80d1d34-b208-4c3d-e652-d186d2262926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are 'i like',\n",
            "\tand the suggested word is `a` with a probability of 0.2727\n",
            "\n",
            "The previous words are 'i like', the suggestion must start with `c`\n",
            "\tand the suggested word is `cat` with a probability of 0.0909\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
      ],
      "id": "n9yyvcv9FveK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04EmionTFveL"
      },
      "source": [
        "#### Get suggestions procedure."
      ],
      "id": "04EmionTFveL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8RSfUUrFveL"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "        \n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ],
      "id": "o8RSfUUrFveL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1Q0mVsEFveL",
        "outputId": "c263cd46-d25c-45a8-f67e-1c0f298a4302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are 'i like', the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('a', 0.2727272727272727),\n",
              " ('a', 0.2),\n",
              " ('dog', 0.1111111111111111),\n",
              " ('dog', 0.1111111111111111)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "quadgram_counts = count_n_grams(sentences, 4)\n",
        "qintgram_counts = count_n_grams(sentences, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
        "\n",
        "print(f\"The previous words are 'i like', the suggestions are:\")\n",
        "display(tmp_suggest3)"
      ],
      "id": "I1Q0mVsEFveL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu_kfPo7FveM"
      },
      "source": [
        "#### Suggest multiple words using n-grams of varying length."
      ],
      "id": "bu_kfPo7FveM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-SWO9ur7FveM",
        "outputId": "b7b63235-6f2e-4ba1-8b6e-c659e324d36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing n-gram counts with n = 1 ...\n",
            "Computing n-gram counts with n = 2 ...\n",
            "Computing n-gram counts with n = 3 ...\n",
            "Computing n-gram counts with n = 4 ...\n",
            "Computing n-gram counts with n = 5 ...\n"
          ]
        }
      ],
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
        "    n_model_counts = count_n_grams(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "id": "-SWO9ur7FveM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR00jPKSFveN",
        "outputId": "1a20bc33-3412-478e-a3f1-3de6ff4878e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('<unk>', 0.027184657967576117),\n",
              " ('have', 0.0001128158844765343),\n",
              " ('have', 0.00011284134506883322),\n",
              " ('i', 5.6427039837490125e-05)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"i\", \"am\", \"to\"]\n",
        "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest4)"
      ],
      "id": "nR00jPKSFveN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMrJlihCFveN",
        "outputId": "7b7ac43f-1531-470f-90d9-bc484c117954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('to', 0.0117453231292517),\n",
              " ('to', 0.003947953736654805),\n",
              " ('to', 0.0007886879612416202),\n",
              " ('to', 0.0003383522246658772)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
        "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest5)"
      ],
      "id": "tMrJlihCFveN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzBzrN6CFveN",
        "outputId": "0df9fc15-1d14-4962-9d34-627b19ba8222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('you', 0.018996062992125983),\n",
              " ('you', 0.002698296700207994),\n",
              " ('you', 0.00011284771201263894),\n",
              " ('i', 5.6427039837490125e-05)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
        "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest6)"
      ],
      "id": "OzBzrN6CFveN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCw0V4mLFveN",
        "outputId": "af3adfa6-4e72-446c-ac6c-a82622ab8908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('are', 0.01566902087916329),\n",
              " ('?', 0.00226431766720053),\n",
              " ('?', 0.0012943891046204063),\n",
              " ('<e>', 0.00011284771201263894)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
        "tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest7)"
      ],
      "id": "LCw0V4mLFveN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLbLwsirFveO",
        "outputId": "8e01cfd3-c091-4045-dde7-2b0ac07a5628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(\"don't\", 0.004168113928447377),\n",
              " ('doing', 0.0013806815043905672),\n",
              " ('doing', 0.0003939445101018628),\n",
              " ('dvd', 5.642385600631947e-05)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
        "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest8)"
      ],
      "id": "gLbLwsirFveO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYXgEdfxFveO"
      },
      "source": [
        "# References\n",
        "\n",
        "Coursera - Natural language processing with probabilistic models - [coursra](https://www.coursera.org/learn/probabilistic-models-in-nlp)."
      ],
      "id": "mYXgEdfxFveO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "auto_complete.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}